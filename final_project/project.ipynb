{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation + '»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "        \n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avito = os.listdir('Avito')\n",
    "avito = ['Avito' + os.sep + a for a in avito]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "avito.remove('Avito/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(avito):\n",
    "    inv_idx_result = []\n",
    "    lengths = {}\n",
    "    ads_texts = []\n",
    "\n",
    "    for a in tqdm_notebook(avito):\n",
    "        with open(a, 'r', encoding='utf-8') as f:\n",
    "            f = f.read()\n",
    "            ads_texts.append(f)\n",
    "\n",
    "        avito_text = preprocessing(f)\n",
    "        lengths[a] = len(avito_text)\n",
    "        inv_idx_result.append(' '.join(avito_text))\n",
    "\n",
    "    return inv_idx_result, lengths, ads_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2f158a00ba4373ae53771dd0343d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_idx_res, l, f = prepare_data(avito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_texts = [i.replace('\\xa0', '') for i in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "avito_texts = []\n",
    "for index, text in enumerate(ads_texts):\n",
    "    avito_texts.append({'avito_text': text, 'index': index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ll = dict(enumerate(l.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgdl = sum(l.values()) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.3769"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ads = dict(enumerate(avito))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inv_index(ii_data, ads):\n",
    "    count = CountVectorizer()\n",
    "    X = count.fit_transform(ii_data)\n",
    "    key = [k for k in ads.keys()]\n",
    "    term_doc_matrix = pd.DataFrame(X.toarray(), index=key, columns=count.get_feature_names())\n",
    "    term_doc_matrix = [term_doc_matrix.index.tolist(), term_doc_matrix.columns.tolist(), term_doc_matrix.values.tolist()]\n",
    "    \n",
    "    return term_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_doc_matrix = get_inv_index(inv_idx_res, ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inv_idx(term_doc_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    count_idf = {}\n",
    "    ads = term_doc_matrix[0]\n",
    "    words = term_doc_matrix[1]\n",
    "    freq = term_doc_matrix[2]\n",
    "    N = len(ads)\n",
    "\n",
    "    for i, j in enumerate(words):\n",
    "        n = 0\n",
    "        for f in freq:\n",
    "            count = f[i]\n",
    "            if count != 0:\n",
    "                n += 1\n",
    "\n",
    "        idf = log((N - n + 0.5) / (n + 0.5))\n",
    "        count_idf[j] = idf\n",
    "    \n",
    "    return count_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_idf = get_inv_idx(term_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "k1 = 2.0\n",
    "b = 0.75\n",
    "\n",
    "def score_BM25(idf, qf, dl, avgdl, k1, b) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    return idf * (k1 + 1) * qf / (qf + k1 * (1 - b + b * dl / avgdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_result(query, n_results) -> list:\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    inv_idx_result = []\n",
    "    res_text = []\n",
    "    \n",
    "    for q in query:\n",
    "        if q in term_doc_matrix[1]:\n",
    "            idx = term_doc_matrix[1].index(q)\n",
    "            for i, a in enumerate(term_doc_matrix[0]):\n",
    "                qf = term_doc_matrix[2][i][idx]\n",
    "                dl = ll[a]\n",
    "                idf = c_idf[q] \n",
    "                okapi = score_BM25(idf, qf, dl, avgdl, k1, b)\n",
    "                inv_idx_result.append({'index': a, 'okapi_score': okapi})\n",
    "    \n",
    "    res_inv_index = sorted(inv_idx_result, key=lambda k: k['okapi_score'], reverse=True)[:n_results]\n",
    "    for i in res_inv_index:\n",
    "        for j in avito_texts:\n",
    "            if i['index'] == j['index']:\n",
    "                res_text.append(j['avito_text'])\n",
    "        \n",
    "    \n",
    "    return res_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "import numpy as np \n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'araneum_none_fasttextskipgram_300_5_2018.model'\n",
    "w2v_model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors(model, input_data):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    vectors = []\n",
    "    \n",
    "    for input_d in input_data:\n",
    "        try:\n",
    "            vector = model.wv[input_d]\n",
    "            vectors.append(vector)\n",
    "        except KeyError as e:\n",
    "            continue\n",
    "            \n",
    "    mean = sum(vectors) / len(vectors)        \n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_w2v_base(avito):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    w2v_result = []\n",
    "    avito_vector = {}\n",
    "    \n",
    "    for a in tqdm_notebook(avito):\n",
    "        with open(a, 'r', encoding='utf-8') as f:\n",
    "            f = f.read()\n",
    "            lemmas = preprocessing(f)\n",
    "            w2v_vectors = get_w2v_vectors(w2v_model, lemmas)\n",
    "            \n",
    "            avito_vector = {'avito_text': f, 'w2v_vectors': w2v_vectors.tolist()}\n",
    "            w2v_result.append(avito_vector)\n",
    "    \n",
    "    return w2v_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac79700d90641289c41345df9701d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w2v_res = save_w2v_base(avito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_w2v(query, model, w2v_res, n_results):\n",
    "    result = {}\n",
    "    final_results = []\n",
    "    get_vectors = get_w2v_vectors(w2v_model, query)\n",
    "    for w2v_r in w2v_res:\n",
    "        compare_similarity = similarity(get_vectors, w2v_r['w2v_vectors'])\n",
    "        result[compare_similarity] = w2v_r['avito_text']\n",
    "        \n",
    "    for res in sorted(result, reverse=True)[:n_results]:\n",
    "        final_results.append(result[res])\n",
    "        \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from judicial_splitter import splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_d2v(avito):\n",
    "    avito_text = {}\n",
    "    d2v_paragraphs = []\n",
    "    \n",
    "    for a in tqdm_notebook(avito):\n",
    "        with open(a, 'r', encoding='utf-8') as f:\n",
    "            f = f.read()\n",
    "            avito_text[a] = f\n",
    "            splitted_text = splitter(f, 1)\n",
    "            \n",
    "            for text in splitted_text:\n",
    "                lemmas = preprocessing(text, del_stopwords=False)\n",
    "                d2v_paragraphs.append({'avito_text': f, 'avito_lemmas': lemmas})\n",
    "    \n",
    "    return avito_text, d2v_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c0cf59f23b4622aa37de9fbb720ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2v_a, d2v_p = split_d2v(avito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(data):\n",
    "    d2v_data = [TaggedDocument(words=j['avito_lemmas'], tags=[str(i)])for i, j in enumerate(data)]\n",
    "\n",
    "    model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.025, min_count=0, workers=4, epochs=100)\n",
    "    model.build_vocab(d2v_data)\n",
    "    model.train(d2v_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2v_model = train_doc2vec(d2v_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(model, input_data):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    d2v_vectors = model.infer_vector(input_data)\n",
    "    \n",
    "    return d2v_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_d2v_base(d2v_data):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    d2v_result = []\n",
    "    avito_vector = {}\n",
    "    \n",
    "    for dictionary in tqdm_notebook(d2v_data):\n",
    "        d2v_vectors = get_d2v_vectors(d2v_model, dictionary['avito_lemmas'])\n",
    "        avito_vector = {'avito_text': dictionary['avito_text'], 'd2v_vectors': d2v_vectors.tolist()}\n",
    "        d2v_result.append(avito_vector)\n",
    "    \n",
    "    return d2v_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d10f9e870b54f468e0c0cb3f2bf12cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2v_res = save_d2v_base(d2v_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_d2v(query, model, d2v_res, n_results):\n",
    "    result = {}\n",
    "    final_results = []\n",
    "    get_vectors = get_d2v_vectors(d2v_model, query)\n",
    "    for d2v_r in d2v_res:\n",
    "        compare_similarity = similarity(get_vectors, d2v_r['d2v_vectors'])\n",
    "        result[compare_similarity] = d2v_r['avito_text']\n",
    "        \n",
    "    for res in sorted(result, reverse=True)[:n_results]:\n",
    "        final_results.append(result[res])\n",
    "        \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, search_method, n_results=5):\n",
    "    if search_method == 'inverted_index':\n",
    "        query = preprocessing(query)\n",
    "        result = get_search_result(query, n_results)\n",
    "        print('\\n\\n'.join(result))\n",
    "    \n",
    "    elif search_method == 'word2vec':\n",
    "        query = preprocessing(query)\n",
    "        result = search_w2v(query, w2v_model, w2v_res, n_results)\n",
    "        print('\\n\\n'.join(result))\n",
    "    \n",
    "    elif search_method == 'doc2vec':\n",
    "        query = preprocessing(query, del_stopwords=False)\n",
    "        result = search_d2v(query, d2v_model, d2v_res, n_results)\n",
    "        print('\\n\\n'.join(result))\n",
    "    \n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    \n",
    "#     return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Игры для компьютера 13 шт\n",
      "Date: № 931252284, размещено 1 октября в 21:10\n",
      "Price: 300₽\n",
      "Address: м.Выхино\n",
      "Игры для компьютера 13 шт\n",
      "\n",
      "Title: Игры на компьютер\n",
      "Date: № 672540624, размещено 29 сентября в 14:05\n",
      "Price: 100₽\n",
      "Address: м.Коньково\n",
      "Продаю 39 игровых диска для компьютора .\n",
      "\n",
      "Title: Игры для компьютера\n",
      "Date: № 1476190446, размещено 4 октября в 07:14\n",
      "Price: 20₽\n",
      "Address: м.Бабушкинская\n",
      "Игры для компьютера. 20р. за диск\n",
      "\n",
      "Title: Программы для компьютера\n",
      "Date: № 1279195158, размещено 5 октября в 07:05\n",
      "Price: 50₽\n",
      "Address: м.Строгино\n",
      "Программы для компьютера Цена /шт.\n",
      "\n",
      "Title: Игры для компьютера. Разные\n",
      "Date: № 197957059, размещено 2 октября в 16:21\n",
      "Price: 100₽\n",
      "Address: м.Бунинская аллея\n",
      "Игры для компьютера.Разные. Лицензия.\n"
     ]
    }
   ],
   "source": [
    "search('компьютер', 'inverted_index', n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Установочный диск Windows 7\n",
      "Date: № 1212351184, размещено 2 октября в 11:52\n",
      "Price: 250 ₽\n",
      "Address: м. Ховрино\n",
      "Загрузочный диск Windows 7 Professional x64 без вирусов, троянов, чистая установка.Официальная русская версия x64 (64 бит), установочный диск Windows 7 без вирусов, троянов и мусорных программ, но без красочных обложек и коробок. Любой способ активации, в т.ч. покупка лицензии (ключа), если нужно продам.*******************Так же делаю загрузочные флешки,- устанавливаю Windows, драйвера, программы- обучаю -  есть оригинальные программы Office, AutoCad, 3dsMAX, Photoshop, наборы программ по вашему списку на заказ и полные комплекты драйверов для ноутбуков и компьютеров- настраиваю ноутбук, компьютер- очищаю от вирусов, троянов и другой нечисти- настраиваю WIFI роутер- сборка компьютера- рем0нт компьютеров ноутбуков любой сложностиЗвоните договаривайтесь о встрече или выполнении работы по удаленке.\n",
      "\n",
      "Title: Windows 10 pro / флешка\n",
      "Date: № 1619530383, размещено 30 сентября в 09:40\n",
      "Price: 2 000 ₽\n",
      "Address: м. Щелковская\n",
      "Продаю лицензионную Windows 10 pro на флешке.Так же, могу бесплатно помощь установить на Ваш компьютер,ноутбук,нетбук,моноблок с полными настройками.\n",
      "\n",
      "Title: Игры диски для компьютера, приставки\n",
      "Date: № 805448979, размещено 7 октября в 02:13\n",
      "Price: 50 ₽\n",
      "Address: м. Владыкино\n",
      "Диски с играми для компьютера и приставки\n",
      "\n",
      "Title: Установочный диск Windows 10\n",
      "Date: № 1453225484, размещено 3 октября в 10:04\n",
      "Price: 250 ₽\n",
      "Address: м. Ховрино\n",
      "Windows 10 Professional Russian x64 x86 загрузочный диск.Установочный диск (Win 7, Win 8, или Win 10) без вирусов троянов и мусора.250-350 р.Драйверы lan wi-fi комплект под все мат. платы и сетевые карты.150-200 р.Официальная русская версия x64 (64 бит), установочный диск Windows 10 без вирусов, троянов и мусорных программ, но без красочных обложек и коробок. Любой способ активации, в т.ч. покупка лицензии (ключа), если нужно продам.Так же делаю загрузочные флешки, устанавливаю Windows при встрече +500 руб, восстановление данных +500 руб., обучение +500 руб., есть оригинальные программы Office, AutoCad, 3dsMAX, Photoshop, наборы программ по вашему списку на заказ и полные комплекты драйверов для ноутбуков и компьютеров.Звоните договаривайтесь о встрече или выполнении работы по удаленке.\n",
      "\n",
      "Title: Касперский Антивирус\n",
      "Date: № 1290657587, размещено 7 октября в 22:39\n",
      "Price: 500 ₽\n",
      "Address: м. Площадь революции\n",
      "Kaspersky Internet Security 2018Дата окончания действия ключа 30.07.2019Купил на 2 компьютера, активировал только на одном.Покупал по акции, поэтому выгоднее было взять на 3 компьютера.Надежная защита Вашего компьютера от интернет угроз.\n"
     ]
    }
   ],
   "source": [
    "search('компьютер', 'word2vec', n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Witcher 2 Premium Edition Ведьмак 2 убийцы королей\n",
      "Date: № 645523853, размещено 30 сентября в 22:15\n",
      "Price: 3 000 ₽\n",
      "Address: м. ВДНХ\n",
      "Новое. Запечатанное. Для Компьютера.\n",
      "\n",
      "Title: Игры для компьютера. Разные\n",
      "Date: № 197957059, размещено 2 октября в 16:21\n",
      "Price: 100 ₽\n",
      "Address: м. Бунинская аллея\n",
      "Игры для компьютера.Разные. Лицензия.\n",
      "\n",
      "Title: Игры на компьютер\n",
      "Date: № 1436509728, размещено 30 сентября в 16:03\n",
      "Price: 1 ₽\n",
      "Address: м. Библиотека им. Ленина\n",
      "Игры на компьютер.Недорого.За ценой и вопросами в сообщения.\n",
      "\n",
      "Title: Tom Clancy's Splinter Cell: Blacklist\n",
      "Date: № 1072511172, размещено 3 октября в 07:22\n",
      "Price: 1 500 ₽\n",
      "Address: м. Бабушкинская\n",
      "24-сантиметровая фигурка Сэма ФишераСтилбукКоллекционная коробка96-страничная графическая новелла Splinter Cell EchoesCollector's edition. Возможен обмен\n",
      "\n",
      "Title: Sony PS2\n",
      "Date: № 1520629317, размещено 7 октября в 14:22\n",
      "Price: 5 500 ₽\n",
      "Address: м. Профсоюзная\n",
      "В хорошем состоянии, с проводами, к телевизору подключатся, с 2 джойстиками и с 5 играми. Шрек, агент 007, Гарри Поттер , Гладиаторы и need for speed. Не все диски в упаковках, но все в рабочем состоянии. Оба джойстика в рабочем состоянии. За подробностями звоните, пишите. Торг возможен\n"
     ]
    }
   ],
   "source": [
    "search('компьютер', 'doc2vec', n_results=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
